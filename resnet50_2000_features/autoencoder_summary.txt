File: autoencoder_sweep.py (2000 Sample Version)

Objective: To test how much we can compress the detailed "math fingerprints" (2048 features) of images without losing their meaning. 
Method:
    - Input: Takes 2,000 image fingerprints extracted by ResNet50.
    - Compression (Autoencoder): Forces the data through a bottleneck of varying sizes: [1024, 512, 256, 128, 64, 32].
    - Evaluation (Classifier): specific "Guesser" (Classifier) tries to identify the image class (e.g., Dog vs. Cat) using only the compressed summary.

Part 2: The Results Analysis
The experiment produced a machine learning result known as the "Goldilocks Zone."

1. Left Graph: Reconstruction Loss
Trend: The line surprisingly goes UP (worse) as dimensions increase.
    - At 32 Dimensions: The loss is low (Good).
    - At 1024 Dimensions: The loss is high (Bad).
Reason: The model with 1024 dimensions is too complex. Because we limited training to 20 epochs, the large model "ran out of time" to learn properly, leading to higher error. The smaller models (32, 64) were simpler and learned faster.

2. Right Graph: Classification Accuracy
Trend: It forms a Bell Curve.
    - At 32 Dims (Left): Accuracy is low (~58%). The compression was too aggressive; we lost critical details (like cat ears vs. dog ears).
    - At 1024 Dims (Right): Accuracy drops (~63%). Because the Autoencoder failed to learn a good summary (see Left Graph), the Classifier got "garbage" input.
    - At 128-256 Dims (Middle): Peak Accuracy (~72.5%).
Conclusion: This is the sweet spot. Successfully compressed the data by 94% (from 2048 down to 128) while actually maximizing performance. This proves that 2048 dimensions contained a lot of "noise" that wasn't needed for classification.